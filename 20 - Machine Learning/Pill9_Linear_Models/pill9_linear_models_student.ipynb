{"cells":[{"cell_type":"markdown","metadata":{"id":"7Axw1dutbWRW"},"source":["# Pill 9 - Linear models\n","\n","Outline:\n","\n","+ Ordinary least squares regression\n","+ Logistic regression\n","+ Generalized additive models\n","+ Support Vector Machines\n"]},{"cell_type":"markdown","metadata":{"id":"YZykZ93AbWRe"},"source":["# Ordinary least squares regression\n","\n","We have seen linear models before several times. Let us recall what they are about. The underlying model is a linear one, this is\n","\n","$$h(x) = a^Tx + b$$\n","\n","In the context of regression, the use of this model can be associated to the minimization of the squared mean error loss, this is\n","\n","$$\n","\\begin{align}\n","\\underset{a,b}{\\text{minimize}} &\\quad \\sum_i (y_i - h(x_i;a,b))^2\n","\\end{align}$$\n","\n","\n","As we saw in other notebooks, this problem can be easily solved using iterative approaches or even using a closed form solution.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HgPVIzTpbWRg"},"source":["# Logistic regression\n","\n","Logistic regression is a **linear classifier** that is based on maximum likelihood principles to jointly model the classification boundary and the certainty of the fit. It is based on fitting a logistic function to the data.\n","\n","$$p(x_i|y) = \\frac{1}{1+e^{-(a^Tx+b)}}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNGW1I__bWRi"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","beta= 0.5\n","x = np.linspace(-10,10,100)\n","p = 1./(1+np.exp(-beta*x)) \n","plt.plot(p)"]},{"cell_type":"markdown","metadata":{"id":"Uf6qpRu3bWRl"},"source":["<div class = \"alert alert-success\" style = \"border-radius:10px\"> **EXERCISE: ** What are hypothesis with respect to the probability modeling?</div>"]},{"cell_type":"markdown","metadata":{"id":"urWatBjHbWRm"},"source":["<div class = \"alert alert-success\" style =\"border-radius:10px\">**EXERCISE: ** Change the value of $\\beta$ and describe its effect.</div>"]},{"cell_type":"markdown","metadata":{"id":"6O-dBzeGbWRo"},"source":["The likelihood function can be written as\n","\n","$$p(x|y) = \\prod\\limits_{i=1}^N p(x_i|y)^{y_i}(1-p(x_i|y))^{1-y_i}$$\n","\n","and the goal is to \n","\n","$$\\text{maximize} \\quad p(x|y)$$\n","\n","Taking the logarithm, we can equivalently maximize\n","\n","$$\\text{maximize} \\quad \\sum_i y_i \\ln p(x_i|y) + (1-y_i) \\ln(1-p(x_i|x))$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aH7DmidsbWRp"},"source":["We can model the probability $p(x_i|x)$ with a logit function, i.e.\n","\n","$$p(x_i|y) = \\ell(x_i) =  \\frac{1}{1+e^{-(a^Tx+b)}}$$"]},{"cell_type":"markdown","metadata":{"id":"wNbqns4ybWRr"},"source":["In order to optimize this model let us write the derivative of the logit function,\n","\n","$$\\frac{\\partial \\ell}{\\partial a} =  (1+e^{-(a^Tx+b)})^{-2}e^{-(a^Tx+b)}x$$\n","$$\\frac{\\partial \\ell}{\\partial b} =  (1+e^{-(a^Tx+b)})^{-2}e^{-(a^Tx+b)}$$\n","\n","\n","In terms of the loss function the gradient with respect to the parameters is\n","\n","$$\\nabla_a \\mathcal{L} = \\sum_i x_i \\ell(x_i)(y_i e^{-(a^Tx_i+b)} - (1-y_i) )  $$\n","$$\\nabla_b \\mathcal{L} = \\sum_i  \\ell(x_i)(y_i e^{-(a^Tx_i+b)} - (1-y_i) )  $$"]},{"cell_type":"markdown","metadata":{"id":"5I7V2kADbWRs"},"source":["Let us solve this using gradient descent:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN0DrfmZbWRt"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[1,-0.9],[-0.9,1]]\n","m2 = [3.,6.]\n","s2 = [[1,0],[0,1]]\n","m3 = [4.,-1.]\n","s3 = [[1,0.5],[0.5,1]]\n","print (s3)\n","c1 = np.random.multivariate_normal(m1,s1,100)\n","c2 = np.random.multivariate_normal(m2,s2,100)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[np.zeros(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5oT5eGubWRv"},"outputs":[],"source":["#gradient descent\n","\n","def logit(x,a,b):\n","    return 1./(1+np.exp(-(np.sum(a*x,axis=1)+b)))\n","\n","def evaluate_objective(x,y,a,b):\n","    return np.sum(y*np.log(logit(x,a,b))+(1-y)*np.log((1-logit(x,a,b))))\n","    \n","def partial_a_objective(x,y,a,b):\n","    partial = x*np.tile((y*np.exp(-(np.sum(a*x,axis=1)+b))*logit(x,a,b)-(1-y)*logit(x,a,b))[:,np.newaxis],(1,2))\n","    return np.sum(partial,axis=0)\n","\n","def partial_b_objective(x,y,a,b):\n","    partial = (y*np.exp(-(np.sum(a*x,axis=1)+b))*logit(x,a,b)-(1-y)*logit(x,a,b))\n","    return np.sum(partial)\n","\n","\n","n_iters = 1000\n","nu=0.01\n","a = np.random.rand(1,x.shape[1])\n","b = np.random.rand(1)\n","conv = []\n","for i in range(n_iters):\n","    at = a + nu*partial_a_objective(x,y,a,b) #maximize\n","    b = b + nu*partial_b_objective(x,y,a,b) #maximize\n","    a = at\n","    conv.append(evaluate_objective(x,y,a,b))\n","    \n","\n","plt.plot(conv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8BnHzE2bWRw"},"outputs":[],"source":["print (a,b)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a94yX8XsbWRx"},"outputs":[],"source":["#my code\n","xx,yy = np.meshgrid(np.linspace(-3,7,200),np.linspace(-4,10,200))\n","viz=np.c_[xx.ravel(),yy.ravel()]\n","\n","\n","z = logit(viz,a,b)\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","plt.gca().set_xlim([-3.,7.])\n","plt.gca().set_ylim([-4.,10.])\n","plt.imshow(z.reshape((200,200)), interpolation='bilinear', origin='lower', extent=(-3,7,-4,10),alpha=0.3, vmin=0, vmax=1)\n","plt.contour(xx,yy,z.reshape((200,200)))\n","\n","plt.gcf().set_size_inches((10,10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pI3K93gWbWRz"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[1,-0.9],[-0.9,1]]\n","m2 = [1.,2.]\n","s2 = [[1,0],[0,1]]\n","c1 = np.random.multivariate_normal(m1,s1,100)\n","c2 = np.random.multivariate_normal(m2,s2,100)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[np.zeros(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUSSXyfabWR0"},"outputs":[],"source":["#gradient descent\n","\n","def logit(x,a,b):\n","    return 1./(1+np.exp(-(np.sum(a*x,axis=1)+b)))\n","\n","def evaluate_objective(x,y,a,b):\n","    return np.sum(y*np.log(logit(x,a,b))+(1-y)*np.log((1-logit(x,a,b))))\n","    \n","def partial_a_objective(x,y,a,b):\n","    partial = x*np.tile((y*np.exp(-(np.sum(a*x,axis=1)+b))*logit(x,a,b)-(1-y)*logit(x,a,b))[:,np.newaxis],(1,2))\n","    return np.sum(partial,axis=0)\n","\n","def partial_b_objective(x,y,a,b):\n","    partial = (y*np.exp(-(np.sum(a*x,axis=1)+b))*logit(x,a,b)-(1-y)*logit(x,a,b))\n","    return np.sum(partial)\n","\n","\n","n_iters = 10000\n","nu=0.01\n","a = np.random.rand(1,x.shape[1])\n","b = np.random.rand(1)\n","conv = []\n","for i in range(n_iters):\n","    at = a + nu*partial_a_objective(x,y,a,b) #maximize\n","    b = b + nu*partial_b_objective(x,y,a,b) #maximize\n","    a = at\n","    conv.append(evaluate_objective(x,y,a,b))\n","    \n","\n","plt.plot(conv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMivGf5-bWR1"},"outputs":[],"source":["#my code\n","xx,yy = np.meshgrid(np.linspace(-3,6,200),np.linspace(-4,6,200))\n","viz=np.c_[xx.ravel(),yy.ravel()]\n","\n","\n","z = logit(viz,a,b)\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","plt.gca().set_xlim([-3.,6.])\n","plt.gca().set_ylim([-4.,6.])\n","plt.imshow(z.reshape((200,200)), interpolation='bilinear', origin='lower', extent=(-3,6,-4,6),alpha=0.3, vmin=0, vmax=1)\n","plt.contour(xx,yy,z.reshape((200,200)),[0.1,0.5,0.9])\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"markdown","metadata":{"id":"76oVLi_1bWR2"},"source":["Let us code this model using CVX"]},{"cell_type":"markdown","metadata":{"id":"MBcCB70pbWR3"},"source":["# Support vector machines\n","Support Vector Machines (SVM) is a prototypical example of discriminative learning. In this setting one explicitly assumes a function model class of the boundary. The classical model for SVM is a linear model. SVM is not the only discriminative linear model, e.g. perceptron, logistic classifier, etc. But, it is probably the most complete problem formulation.\n","\n","Let us first check the intuition behind SVM,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Krxdf0yRbWR3"},"outputs":[],"source":["%matplotlib inline\n","%reset -f\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact\n","\n","class HLA():\n","    def __init__(self):\n","        np.random.seed(1)\n","        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n","        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n","        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n","        plt.scatter(self.X[40:,0],self.X[40:,1],color='b') \n","        delta = 0.025\n","        xx = np.arange(-5.0, 10.0, delta)\n","        yy = np.arange(-5.0, 10.0, delta)\n","        XX, YY = np.meshgrid(xx, yy)\n","        Xf = XX.flatten()\n","        Yf = YY.flatten()\n","        self.sz=XX.shape\n","        self.data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n","\n","    def run(self,w0,w1,offset):\n","        w=np.array([w0,w1])\n","        w.shape=(2,1)\n","        Z = self.data.dot(w)+offset\n","        Z.shape=self.sz\n","        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n","        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n","        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-30, vmax=30)\n","        XX = self.data[:,0].reshape(self.sz)\n","        YY = self.data[:,1].reshape(self.sz)\n","        plt.contour(XX,YY,Z,[0])\n","        fig = plt.gcf()\n","        fig.set_size_inches(9,9)\n","   \n","\n","def decorator(w0,w1,offset):\n","    widget_hla.run(w0,w1,offset)\n","    \n","widget_hla = HLA()\n","interact(decorator, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));"]},{"cell_type":"markdown","metadata":{"id":"CKHsRyu8bWR9"},"source":["<div class = \"alert alert-success\">**QUESTION:** Using the former widget, check manually the following configurations:\n","\n","<li> $(w_0,w_1,\\text{offset}) = (-1.7, -3.1, 10)$\n","<li> $(w_0,w_1,\\text{offset}) = (-3.7, -0.5, 10.3)$\n","<li> $(w_0,w_1,\\text{offset}) = (-7.5, -3.2, 28.8)$\n","<p>\n","Which one of those configuration do you think yields a better boundary? Why?\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"3bcmWOh-bWR-"},"source":["<div class=\"alert alert-info\">\n","**INTUITION:** The Support Vector Machine classifer finds the boundary with maximum distance/**margin** to both classes.</div>"]},{"cell_type":"markdown","metadata":{"id":"hv1Q3S1XbWSB"},"source":["Observations:\n","- It implicitly models the notion of noise. One expects that the boundary with maximum margin will be robust to small perturbations in the data.\n","- A maximum margin classifier has a unique solution in the separable case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0i42EAKbWSC"},"outputs":[],"source":["\n","%reset -f\n","%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact\n","from sklearn import svm\n","\n","class svm_example():\n","    def __init__(self):\n","        '''Data creation'''\n","        np.random.seed(1)\n","        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n","        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n","\n","    def run(self):\n","        '''Fit a linear SVM'''\n","        self.clf = svm.SVC(kernel='linear')\n","        self.clf.fit(self.X,self.y.ravel())\n","        \n","    def display(self):\n","        '''Display stuff'''\n","        delta = 0.25\n","        xx = np.arange(-5.0, 10.0, delta)\n","        yy = np.arange(-5.0, 10.0, delta)\n","        XX, YY = np.meshgrid(xx, yy)\n","        Xf = XX.flatten()\n","        Yf = YY.flatten()\n","        sz=XX.shape\n","        data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n","        Z=self.clf.decision_function(data)\n","        Z.shape=sz\n","        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n","        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n","        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-3, vmax=3)\n","        XX = data[:,0].reshape(sz)\n","        YY = data[:,1].reshape(sz)\n","        plt.contour(XX,YY,Z,[-1,0,1],colors=['b','k','r'])\n","        fig = plt.gcf()\n","        fig.set_size_inches(9,9)\n","        print ('Number of support vectors: ' + str(np.sum(self.clf.n_support_)))\n","        plt.scatter(self.clf.support_vectors_[:, 0], \n","           self.clf.support_vectors_[:, 1], \n","           s=120, \n","           facecolors='none', \n","           linewidths=2,\n","           zorder=10)\n","        print ('(w0,w1) = ' + str(10*self.clf.coef_[0]))\n","        print ('offset = ' + str(10*self.clf.intercept_[0]))\n","        return XX,YY,Z\n","\n","\n","\n","c = svm_example()\n","c.run()\n","XX,YY,Z=c.display()"]},{"cell_type":"markdown","metadata":{"id":"C03CHcq2bWSD"},"source":["Observe that there is a critical subset of data points. These are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model."]},{"cell_type":"markdown","metadata":{"id":"or5Y9X-lbWSD"},"source":["Check the intuition in 3D:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XMmRbJbUbWSD"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","np.random.seed(1)\n","X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n","y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n","def control3D(elevation,azimuth):\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111, projection='3d')\n","    fig.set_size_inches(12,12)\n","    ax.plot_surface(XX,YY,Z,cmap=cm.coolwarm,alpha=0.3,linewidth=0)\n","    ax.scatter(X[0:40,0],X[0:40,1],1,color='r')\n","    ax.scatter(X[40:,0],X[40:,1],-1,color='b')\n","    ax.contour(XX,YY,Z,[-1,0,1],colors=['b','k','r'])\n","    ax.view_init(elev=elevation, azim=azimuth)\n","\n","#Ipython 2.0\n","interact(control3D,elevation=(0.,90.),azimuth=(0,360))\n","#Ipython 1.1\n","#elevation = 45\n","#azimuth = 180\n","#control3D(elevation,azimuth)"]},{"cell_type":"markdown","metadata":{"id":"1KK93XS6bWSE"},"source":["<div class = \"alert alert-success\">**QUESTIONS: **\n","<li> Set the azimuth to $113$ and elevation to $0$. Observe the data points and the relative position of the hyperplane. \n","<li> Change the elevation to $90$. Describe this projection.\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"IyV_UgGfbWSE"},"source":["### 3.1.1 Geometry of the hyperplane\n","A hyperplane in ${\\bf R}^d$ is defined as an affine combination of the variables: $\\pi\\equiv a^Tx + b = 0$. \n","\n","Features:\n","\n","+ A hyperplane splits the space in two half-spaces. The evaluation of the equation of the hyperplane on any element of one of the half-space is a positive value. It is a negative value for all the elements in the other half-space.\n","+ The distance of a point $x \\in{\\bf R}^d$ to the hyperplane $\\pi$ is \n","$$d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$$\n"]},{"cell_type":"markdown","metadata":{"id":"HshjFFB-bWSF"},"source":["### 3.1.2 Modeling the separating hyperplane\n","Given a binary classification problem with training data $\\mathcal{D}=\\{(x_i,y_i)\\},\\; i=1\\dots N, \\; y_i\\in\\{+1,-1\\} $. Consider $\\mathcal{S} \\subseteq \\mathcal{D}$ the subset of all data points belonging to class $+1$, $\\mathcal{S}=\\{x_i | y_i=+1\\}$, and $\\mathcal{R}=\\{x_i | y_i=-1\\}$ its complement. \n","\n","Then the problem of finding a separating hyperplane consists of fulfilling the following constraints\n","\n","$$a^Ts_i+b>0\\; \\text{and}\\; a^Tr_i+b<0 \\quad \\forall s_i\\in\\mathcal{S}, r_i\\in\\mathcal{R}.$$\n","\n","Note the strict inequalities in the formulation. Informally, we can consider the smallest satisfied constraint. And observe that the rest must be satisfied with a larger value. Thus, we can arbitrarily set that value to 1 and rewrite the problem as $$a^Ts_i+b\\geq 1\\; \\text{and}\\; a^Tr_i+b\\leq -1.$$\n","\n","This is a *feasibility problem* and it is usually written in the following way in optimization standard notation\n","\n","$$\n","\\begin{align}\n","\\text{minimize} & 1\\\\\n","\\text{subject to} & a^T r_i + b \\leq -1,\\; \\forall r_i \\in \\mathcal{R}\\\\\n","& a^T s_i + b \\geq 1\\; \\forall s_i \\in \\mathcal{S}\n","\\end{align}\n","$$\n","\n","or in a compact way\n","\n","$$\n","\\begin{align}\n","\\text{minimize} & 1\\\\\n","\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n","\\end{align}\n","$$\n","\n","The solution of this problem is not unique, e.g. remember all the parameters of the 'Human Learning Algorithm'.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sO3j7GHBbWSF"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[1,-0.9],[-0.9,1]]\n","m2 = [3.,6.]\n","s2 = [[1,0],[0,1]]\n","m3 = [4.,-1.]\n","s3 = [[1,0.5],[0.5,1]]\n","print (s3)\n","c1 = np.random.multivariate_normal(m1,s1,100)\n","c2 = np.random.multivariate_normal(m2,s2,100)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[-np.ones(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEr1Lt1NbWSG"},"outputs":[],"source":["from cvxpy import *\n","import numpy\n","\n","d = x.shape[1]\n","N = x.shape[0]\n","\n","# Construct the problem.\n","a = Variable(d)\n","b = Variable()\n","error = 1.\n","obj = Minimize(error)\n","constraints = [multiply(y,x@a+b)>=1]\n","prob = Problem(obj,constraints)\n","prob.solve()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfEA-e8CbWSG"},"outputs":[],"source":["prob.status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4bRXmtSbWSG"},"outputs":[],"source":["a.value, b.value"]},{"cell_type":"markdown","metadata":{"id":"0GGziqA0bWSH"},"source":["<div class = \"alert alert-success\" style = \"border-radius:10px\"> **EXERCISE: ** Fill the blank with the model function to predict the boundary</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cHdLGSqbWSH"},"outputs":[],"source":["#your code\n","xx,yy = np.meshgrid(np.linspace(-3,6,200),np.linspace(-4,6,200))\n","viz=np.c_[xx.ravel(),yy.ravel()]\n","\n","\n","z = #Â PUT HERE YOUR CODE\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","plt.gca().set_xlim([-3.,6.])\n","plt.gca().set_ylim([-4.,6.])\n","plt.imshow(z.reshape((200,200)), interpolation='bilinear', origin='lower', extent=(-3,6,-4,6),alpha=0.3, vmin=-1, vmax=1)\n","plt.contour(xx,yy,z.reshape((200,200)),[0.])\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"markdown","metadata":{"id":"RZ_nzWnQbWSH"},"source":["### 3.1.3 The maximum margin hyperplane\n","\n","Selecting the maximum margin hyperplane requires to add a new constraint to our problem. Remember from the geometry of the hyperplane that the distance of any point to a hyperplane is given by $d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$. \n","\n","Recall that we want positive data to be beyond value 1 and negative data below -1. Thus, what is the distance value we want to maximize?\n","\n","The positive point closest to the boundary is at $1/\\|a\\|_2$ and the negative point closest to the boundary data point is also at $1/\\|a\\|_2$. Thus data points from different classes are at least $2/\\|a\\|_2$ apart. \n","\n","Recall that our goal is to find the separating hyperplane with maximum margin, i.e. with maximum distance among elements from different classes. Thus, we can complete the former formulation with our last requirement as follows\n","\n","$$\n","\\begin{align}\n","\\text{maximize} & \\quad 2/\\|a\\|_2 \\\\\n","\\text{subject to} & \\quad y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n","\\end{align}\n","$$\n","\n","or equivalently,\n","\n","$$\n","\\begin{align}\n","\\text{minimize} & \\quad \\|a\\|_2/2 \\\\\n","\\text{subject to} & \\quad y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n","\\end{align}\n","$$\n","\n","This formulation has a solution as long as the problem is linearly separable."]},{"cell_type":"markdown","metadata":{"id":"morM2VHEbWSJ"},"source":["<div class = \"alert alert-success\">**EXERCISE: ** Code the hard margin SVM.</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9nccjoCbWSK"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[1,-0.9],[-0.9,1]]\n","m2 = [3.,6.]\n","s2 = [[1,0],[0,1]]\n","m3 = [4.,-1.]\n","s3 = [[1,0.5],[0.5,1]]\n","print (s3)\n","c1 = np.random.multivariate_normal(m1,s1,100)\n","c2 = np.random.multivariate_normal(m2,s2,100)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[-np.ones(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuBZLYnAbWSK"},"outputs":[],"source":["#Your code here\n","\n","from cvxpy import *\n","import numpy\n","\n","x = x[0:,:]\n","y = y[0:]\n","\n","d = x.shape[1]\n","N = x.shape[0]\n","\n","# Construct the problem.\n","# Declare variables\n","objective= ###########\n","constraints= ###########\n","\n","\n","prob = Problem(objective,constraints)\n","prob.solve()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VpWbBXEbWSL"},"outputs":[],"source":["a.value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEdDwXHcbWSL"},"outputs":[],"source":["#my code\n","xx,yy = np.meshgrid(np.linspace(-3,6,200),np.linspace(-4,6,200))\n","viz=np.c_[xx.ravel(),yy.ravel()]\n","\n","\n","z = np.dot(viz,a.value)+b.value\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","plt.gca().set_xlim([-3.,6.])\n","plt.gca().set_ylim([-4.,6.])\n","plt.imshow(z.reshape((200,200)), interpolation='bilinear', origin='lower', extent=(-3,6,-4,6),alpha=0.3, vmin=-1, vmax=1)\n","plt.contour(xx,yy,z.reshape((200,200)),[-1.,0.,1.])\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"markdown","metadata":{"id":"3T5suA-CbWSM"},"source":["### 3.1.4 Dealing with the non-separable case\n","\n","In order to deal with misclassifications, we are going to introduce a new set of variables $\\xi_i$, that represents the amount of violation in the $i-th$ constraint. If the constraint is already satisfied, then $\\xi_i=0$, and $\\xi_i>0$ otherwise. Because $\\xi_i$ is related to the errors, we would like to keep this amount as close so zero as possible. This makes us introduce a element in the objective trading-off with the maximum margin.\n","\n","The new model becomes\n","\n","$$\n","\\begin{align}\n","\\text{minimize} & \\|a\\|_2/2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n","\\text{subject to} & y_i (a^T x_i + b) \\geq 1 - \\xi_i,\\; i=1\\dots N\\\\\n","& \\xi_i\\geq 0\n","\\end{align}\n","$$\n","\n","where $C$ is the trade-off parameter that roughly balances margin and misclassification rate. This formulation is also called **soft-margin SVM**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NifY0MU3bWSM"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[1,-0.9],[-0.9,1]]\n","m2 = [1.,2.]\n","s2 = [[1,0],[0,1]]\n","c1 = np.random.multivariate_normal(m1,s1,100)\n","c2 = np.random.multivariate_normal(m2,s2,100)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[-np.ones(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"markdown","metadata":{"id":"Gf-J4dqBbWSM"},"source":["<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE: ** Run the code for the hard margin SVM. Describe the result.</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ekp9fAs1bWSN"},"outputs":[],"source":["#Your code here\n","\n","from cvxpy import *\n","import numpy\n","\n","\n","d = x.shape[1]\n","N = x.shape[0]\n","\n","# Construct the problem.\n","# Declare variables\n","objective= ###########\n","constraints= ###########\n","\n","prob = Problem(objective,constraints)\n","prob.solve()\n","prob.status\n"]},{"cell_type":"markdown","metadata":{"id":"MY53SqR0bWSN"},"source":["<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE: ** Code the soft margin SVM. Describe the result when changing parameter C.</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVvO4onGbWSN"},"outputs":[],"source":["#your code\n","#my code\n","from cvxpy import *\n","import numpy\n","\n","d = x.shape[1] \n","N = x.shape[0]\n","C = 100.\n","\n","# Construct the problem.\n","# Declare variables\n","objective= ###########\n","constraints= ###########\n","\n","prob = Problem(objective,constraints)\n","prob.solve()\n","prob.status\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM-F0B9RbWSO"},"outputs":[],"source":["xx,yy = np.meshgrid(np.linspace(-3,6,200),np.linspace(-4,6,200))\n","viz=np.c_[xx.ravel(),yy.ravel()]\n","\n","\n","z = np.dot(viz,a.value)+b.value\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","plt.gca().set_xlim([-3.,6.])\n","plt.gca().set_ylim([-4.,6.])\n","plt.imshow(z.reshape((200,200)), interpolation='bilinear', origin='lower', extent=(-3,6,-4,6),alpha=0.3, vmin=-1, vmax=1)\n","plt.contour(xx,yy,z.reshape((200,200)),[-1.,0.,1.])\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"markdown","metadata":{"id":"nElLTNUWbWSO"},"source":["<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE: ** Consider the formulation of the soft-margin SVM in a separable case. If one sets parameter $C$ to 0, what is the resutl? do we recover the hard-margin SVM? </div>"]},{"cell_type":"markdown","metadata":{"id":"dfLVwFZebWSO"},"source":["<div class=\"alert alert-info\">**Take home ideas:**\n","<ul>\n","<li> Classical SVM fits a hyperplane separating boundary. </li>\n","<li> The hyperplane is defined to achieve the maximum margin. </li>\n","<li> If the problem is not linearly separable a new term related to the misclassification performance is introduced that trades-off with the margin. This trade-off is governed by parameter $C$ (or $\\nu$ in $\\nu$-SVM). </li>\n","</ul>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"HiF8lkoIbWSP"},"source":["<div class = \"alert alert-danger\" style = \"border-radius:10px\">**DELIVERABLE: ** We want to modify the soft-margin SVM to be able to accomodate unbalanced data. Describe how to change the formulation such that we can control the balance of the errors from each class. Code an example and apply it to the following problem.</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCOK0m8nbWSP"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","m1 = [0.,0.]\n","s1 = [[0.05,0],[0,0.05]]\n","m2 = [1.,2.]\n","s2 = [[1,0],[0,1]]\n","c1 = np.random.multivariate_normal(m1,s1,20)\n","c2 = np.random.multivariate_normal(m2,s2,300)\n","\n","\n","x= np.r_[c1,c2]\n","y= np.r_[-np.ones(c1.shape[0]),np.ones(c2.shape[0])]\n","\n","\n","print (x.shape,y.shape)\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(c1[:,0],c1[:,1],'r.')\n","plt.plot(c2[:,0],c2[:,1],'b.')\n","\n","\n","plt.gcf().set_size_inches((9,9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgUfoA7dbWSP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPhI_4WYbWSQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDENTiy-bWSQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drQHmZQEbWSQ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}