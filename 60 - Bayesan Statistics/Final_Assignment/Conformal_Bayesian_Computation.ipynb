{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from IPython.core.display import HTML"],"metadata":{"id":"yf-FzWZT5OKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Authors: Gerard Castro, Leonardo Bocchi* </br>\n","Public repo: https://github.com/gcastro-98/conformal-bayesian.git\n","<div align=\"center\">\n","  <h1>Bayesian Statistics: Final Project</h1>\n","  <h2>Conformal Bayesian Computation</h2>\n","</div>\n","</br>\n","\n","Conformal Bayesian Computation (CBC) refers to a framework that combines elements of both conformal prediction and Bayesian inference.\n","\n","Conformal prediction is a machine learning approach that provides valid measures of confidence or credibility for predictions made by a model. It constructs prediction regions or sets that contain future observations with a certain probability. These prediction regions are constructed based on the observed training data.\n","\n","On the other hand, Bayesian inference is a statistical framework that provides a way to update prior beliefs about a quantity of interest based on observed data. It involves specifying prior beliefs, likelihood functions, and using Bayes' theorem to obtain posterior distributions.\n","\n","Conformal Bayesian computation combines these two approaches by using conformal prediction to provide valid measures of uncertainty within a Bayesian framework. It leverages the flexibility and interpretability of Bayesian inference while incorporating the notion of confidence or credibility from conformal prediction. By using conformal Bayesian computation, it is possible to obtain probabilistic predictions with well-calibrated uncertainty estimates. This can be useful in various applications, such as regression, classification, and anomaly detection, where it is important to have reliable measures of uncertainty associated with predictions.\n","\n","In this project we follow the work of [Fong, Edwin, and Chris C. Holmes. \"Conformal bayesian computation.\" Advances in Neural Information Processing Systems 34 (2021): 18268-18279](https://arxiv.org/abs/2106.06137), going through the proposed methods and presenting the obtained results. In their work the authors show that CBC can provide accurate and well-calibrated measures of uncertainty in a variety of settings, including Bayesian linear regression, Bayesian logistic regression, and Bayesian neural networks. They also show that CBC can be used to construct credible intervals for the parameters of interest, and that these intervals have good frequentist coverage properties."],"metadata":{"id":"uuKNc_NDnOzf"}},{"cell_type":"markdown","source":["# 1. Introduction"],"metadata":{"id":"H3YOKFmw1NPq"}},{"cell_type":"markdown","source":["CBC constructs prediction intervals or regions that cover the unknown but true posterior distribution of the parameters of interest with a specified probability level."],"metadata":{"id":"TNZYuuIDF_za"}},{"cell_type":"markdown","source":["In general, Bayesian prediction makes use of the posterior predictive distribution being of the form\n","\n","$$p(y|x_{n+1},Z_{1:n}) = \\int f_{\\theta}(y|x_{n+1})\\pi(\\theta |Z_{1:n})d\\theta$$\n","\n","were $\\pi(\\theta |Z_{1:n})$ is the Bayesian posterior, and $f_{\\theta}(y|x)$ is the model likelihood. Given this Bayesian predictive distribution it is possible to construct the highest density posterior predictive credible interval as well as the central credible interval.\n","However, it is well known Bayesian intervals can be poorly calibrated in the frequentist sense if that model presents misspecification. </br>\n","Conformal inference is proposed as a solution to this issue as a way to \"de-Bayesing\" and \"conformalizing\". </br> In their work, Edwin Fong and Chris Holmes present a scalable Monte Carlo (MC) method for *conformal Bayes* using an 'add-one-in' importance sampling algorithm, that only makes use of samples of model parameter values from the posterior $\\theta \\sim \\pi(\\theta|Z_{1:n})$. The authors demonstrate how \"the Bayesian hierarchical model allows for a natural sharing of information between\n","groups for within-group predictions with covariates\". They do so by developing computationally efficient methods in this settings."],"metadata":{"id":"j8iAxDS71nXI"}},{"cell_type":"markdown","source":["## 1.1 Background"],"metadata":{"id":"1Ir6gyiF6PQ1"}},{"cell_type":"markdown","source":["Conformal inference methods were first introduced by Gammerman et al. (1998), but full conformal prediction methods are not viable in terms of computational efficiency, requiring the retraining of the model at each test covariate $x_{n+1}$ and for each value in a reference grid of potential outcomes. Other techniques have been proposed in order to reduce the computational cost, such as test split confromal prediction, and cross-conformal prediction; \"a detailed\n","discussion of computational costs of various conformal methods is provided in Barber et al. (2021,\n","Section 4)\"."],"metadata":{"id":"xDdPNABj6Urc"}},{"cell_type":"markdown","source":["# 2. Conformal Bayes"],"metadata":{"id":"QdqHQbYAH6UH"}},{"cell_type":"markdown","source":["## 2.1 Full Conformal Prediction"],"metadata":{"id":"2VmYPtdhIAUn"}},{"cell_type":"markdown","source":["Conformal prediction is a framework for constructing prediction intervals that come with guarantees of validity. The key idea is to use only the data to construct the intervals, so they are therefore \"conformal\" to the data.\n","The Full Conformal Prediction (FCP) approach consists of the following steps:\n","\n","Define a conformity measure $\\sigma$ (for instance the negative squared error for point prediction) and produces the following $1-α$ confidence prediction interval:\n","\n","$$C_α(Xn+1) = {y ∈ R : π(y) > α}$$\n","\n","where $π(y)$ is the rank of $σ_{n+1}$ among $σ_{1:n+1}$ and it is defined as\n","\n","$$π(y) = \\frac{1}{n+1}\\sum^{n+1}_{i=1} \\mathbb{1}(σ_{i}\\leq σ_{n+1})$$\n","\n","and $σ_i := σ(Z_{1:n+1}| Z_i)$ is the conformity measure for the sample i"],"metadata":{"id":"pPmI06yjH3h5"}},{"cell_type":"markdown","source":["## 2.2 Conformal Bayes and Add-One-In Importance Sampling"],"metadata":{"id":"MmQmioF6LheF"}},{"cell_type":"markdown","source":["The method adopting as the conformity score the natural suggestion of the posterior predictive density\n","\n","$$σ(Z_{1:n+1};Z_i) = p(Y_i| X_i, Z_{1:n+1})$$\n","\n","is denoted as Conformal Bayes (CB). The authors' work highlights a crucial insight: the refitting of the Bayesian model using $${Z_1, . . . , Z_n, \\{y, X_{n+1}\\}}$$ can be effectively approximated through Importance Sampling (IS), where only $\\{y, X_{n+1}\\}$ changes between refits. This approach allows to ease computational efforts and make this efficient to compute predictive densities. This realization directly leads to an IS-based approach for achieving full conformal Bayes, wherein the computation of \"Add-One-In\" (AOI) predictive densities becomes crucial. The term \"AOI\" signifies the inclusion of $\\{Y_{n+1}, X_{n+1}\\}$ into the training set, drawing parallels to the concept of \"leave-one-out\" (LOO) cross-validation.\n","\n","The use of AOI importance sampling exhibits similarities with the computation of Bayesian leave-one-out cross-validation (LOOCV) predictive densities (Vehtari et al., 2017), which is also employed to account for model misspecification. However, an interesting aspect of AOI, in comparison to LOO, is its ability to generate predictive densities that are less susceptible to instability in the importance weights.\n","\n","\n","\n","\n"],"metadata":{"id":"VqCYyNFmL4sN"}},{"cell_type":"markdown","source":["## 2.4 Motivation"],"metadata":{"id":"C9g8-eWoN2DP"}},{"cell_type":"markdown","source":["The motivation/purpose of the authors' work is stated as follows:\n","\n","Considerable literature has delved into the differing foundations and interpretations of uncertainty measures between Bayesian and frequentist approaches (Little, 2006; Shafer and Vovk, 2008; Bernardo and Smith, 2009; Wasserman, 2011). A summary of these discussions can be found in the Appendix. In this context, we present the motivation for Conformal Bayesian (CB) predictive intervals from both Bayesian and frequentist perspectives.\n","\n","Pragmatic Bayesians, who acknowledge the potential for model misspecification in either the prior or likelihood, may find value in employing conformal inference as a safeguard. CB predictive intervals, which guarantee frequentist coverage, can be offered as a complement to the traditional Bayesian predictive intervals. Moreover, the disparity between the Bayesian and conformal intervals can serve as an informal tool for model evaluation (e.g., Gelman et al., 2013). Given that posterior samples obtained through Markov chain Monte Carlo (MCMC) or direct sampling are typically accessible, CB using automated AOI incurs minimal additional computational overhead\n","\n","The frequentist perspective also offers the possibility of utilizing a Bayesian model as a valuable tool for constructing predictive confidence intervals. There are several advantages to this approach. Firstly, unlike the traditional residual conformity score, the likelihood function in the Bayesian model can account for factors such as skewness and heteroscedasticity. This allows for a more comprehensive representation of the data. Secondly, incorporating features like sparsity, support, and regularization can be achieved through the use of priors in the Bayesian model. This flexibility allows for the inclusion of additional information and prior beliefs, while ensuring that Conformal Bayesian (CB) methodology maintains correct coverage of the predictive intervals.\n","\n","However, a subtle issue arises in full conformal prediction where validity is compromised if hyperparameter selection is not symmetric with respect to $Z_{n+1}$. For example, if the lasso penalty $λ$ is estimated using only Z1:n before computing the full conformal intervals with $λ(Z_{1:n})$. In contrast, CB addresses this issue by incorporating a prior distribution on hyperparameters. This prior induces a weighting of hyperparameter values through implicit cross-validation for each refit. This approach is supported by previous research (Gneiting and Raftery, 2007; Fong and Holmes, 2020). It is worth noting that this issue does not affect the split conformal method."],"metadata":{"id":"JniUnt37K6HO"}},{"cell_type":"markdown","source":["## 3.1 Group Conformal Prediction"],"metadata":{"id":"_RDJMV9BeAaV"}},{"cell_type":"markdown","source":["We now introduce the concept of Group Conformal Prediction (GCP), which is a modification to CBC that allows for prediction sets that cover the true posterior distribution of the parameters within each group with a specified probability level; GCP is a special case of PCP.\n","\n","GCP consists of the following steps:\n","\n","1. Divide the observed data into groups.\n","Use a Bayesian model to generate a set of $M$ posterior samples $\\{θ^m\\}_{m=1}^M$ for each group, which approximate the posterior distribution of the parameters of interest within each group.\n","2. For each observed data point $y$ in each group, compute the conformal p-value $P(y,θ^m)$ for each posterior sample $θ^m$ within that group, using the same formula as in CBC.\n","3. Construct the prediction set $C(y)$ as $C(y) = {θ : P(y,θ) \\geq \\alpha}$, where $\\alpha$ is the pre-specified significance level, but only for each group separately.\n","Note that GCP can be combined with CBC, CB, and AOIS to handle models with group structure in a wide range of applications."],"metadata":{"id":"DTKpPOV_eA_9"}},{"cell_type":"markdown","source":["# 4. Experiments"],"metadata":{"id":"IM42LynEi7Z_"}},{"cell_type":"markdown","source":["### 4.0. Necessary imports"],"metadata":{"id":"0P86fDolAiMS"}},{"cell_type":"code","source":["import os\n","from os import makedirs\n","from sklearn.datasets import load_diabetes, load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LassoCV, Lasso\n","from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n","import time\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"GOqtbQ2W-Ypo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Google Collab does not allow for the proper envinronment to run the scripts in the Cloud. However, the code will be displayed and the results obtained locally were uploaded so that they can be presented henceforth"],"metadata":{"id":"xfdf6pywBBpU"}},{"cell_type":"markdown","source":["## 4.1 Sparse Regression"],"metadata":{"id":"D-Ycf6VIi9rL"}},{"cell_type":"markdown","source":["The authors apply the CBC framework to sparse regression with the [`sklearn` diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html): a regression when the number of predictors (covariates) is large (d=10) and there  are few samples (n=442).\n","\n","For reproducibility, all the predictors and the response variable (diabetes progression, continuous) were normalized to have mean $0$ and standard deviation $1$. We will compute the central (1−α) credible interval from the Bayesian posterior predictive CDF estimated using Monte Carlo.\n","\n","The Bayesian model considered is: $$\n","\\begin{aligned}\n","& f_\\theta(y \\mid x)=\\mathcal{N}\\left(y \\mid \\theta^{\\mathrm{T}} x+\\theta_0, \\tau^2\\right) \\\\\n","& \\pi\\left(\\theta_j\\right)=\\text { Laplace }(0, b), \\quad \\pi\\left(\\theta_0\\right) \\propto 1, \\quad \\pi(b)=\\operatorname{Gamma}(1,1) \\quad \\pi(\\tau)=\\mathcal{N}^{+}(0, c)\n","\\end{aligned}\n","$$\n","for $j = 1, \\ldots , d$, and where $b$ is the scale parameter and $\\mathcal{N}^{+}$ is the half-normal distribution.\n","\n","In the experiment we will consider two\n","values of $c$ for the hyperprior on $\\tau$, which correspond to:\n","- A **well-specified** prior for this dataset, $c = 1$, according to Jansen, 2013 (Chapter 4.5)\n","- A **poorly-specified** prior (`misspec`), $c = 0.02$, in which the posterior on $\\tau$ will be heavily weighted towards a small value.\n","\n","To check coverage, we repeatedly divide into a training and test dataset for 50 repeats, with 30% of\n","the dataset in the test split. We evaluate the conformal prediction set on a grid of size $n_{grid} = 100$ between $[y_{\\min} − 2, y_{\\max} + 2]$, where $y_{\\min}$, $y_{\\max}$ is computed from each training dataset."],"metadata":{"id":"1z5zOm8oi_CJ"}},{"cell_type":"markdown","source":["### Code"],"metadata":{"id":"evsT-47ECux1"}},{"cell_type":"code","source":["\"\"\"\n","Summary of the Fong & Holmes (2021) implementation on Conformal Bayesian\n","Computation. Specifically, this module deals with the sparse regression\n","(and its uncertainty quantification) assessed for the `sklearn`:\n","- Diabetes dataset (diabetes)\n","\n","The following 4 methods are implemented:\n","- Bayesian inference (bayes)\n","- Conformal bayes (cb)\n","- Split conformal prediction (split)\n","- Full conformal prediction (full)\n","\n","A brief summary of the models can be found below:\n","- 'bayes': This method uses the posterior samples of the model parameters to\n","compute the likelihood of the test point belonging to each class. It uses the\n","likelihoods to construct a prediction interval.\n","- 'split': This method fits a LASSO logistic regression model to the first\n","half of the training data, then computes the residuals on the second half\n","of the training data. It uses the residuals to define a threshold for the\n","prediction intervals, which are constructed using the logistic regression\n","model on the testing data.\n","- 'full': This method fits a logistic regression model to the combined\n","training and testing data, then computes the rank of the test point in\n","the combined data set. It uses the rank to define a threshold for the\n","prediction intervals, which are constructed using the logistic\n","regression model on the test point.\n","- 'cb': uses the posterior distribution from MCMC sampling to define\n","conformal prediction intervals.\n","\n","\n","The execution chain is as it follows:\n","- The data is loaded using `load_train_test_sparse_regression`\n","- The Bayesian inference (bayes) is performed through the function\n","`run_sparse_regression_mcmc` (which requires the MCMC computations\n","defined at `fit_mcmc_laplace`).\n","- Then, using the former results, the Conformal Bayesian method\n","(along the 2 other baselines) is applied using the function\n","`run_sparse_regression_conformal`:\n","  - The 'split' and 'full' conformal baselines are defined in the functions\n","`conformal_split` and `conformal_full`, respectively.\n","  - The 'cb' method is implemented in the function `compute_cb_region_IS`\n","  and it also uses the own `logistic_loglikelihood` clause.\n","- Once launched the main function `run_sparse_regression_conformal`, it\n","loads the training and testing data and the posterior samples of the model\n","parameters (computed because `run_sparse_regression_mcmc` was run first).\n","- Then it iterates through a specified number of repetitions, applying each\n","of the four methods to compute prediction intervals and coverage probabilities\n","for each test point.\n","- For each repetition, the function applies the split method, the full method,\n","the Bayesian method, and the conformal Bayes method to compute the prediction\n","intervals and coverage probabilities. It also records the computation time\n","for each method.\n","- At the end of the function, these results are saved to various files,\n","including the coverage probabilities, the lengths of the prediction\n","intervals, and the computation times for each method.\n","\n","\"\"\"\n","\n","\n","# #############################################################################\n","# LOAD DATA (sparse regression)\n","# #############################################################################\n","\n","def load_train_test_sparse_regression(train_frac, dataset, seed):\n","    # Load dataset\n","    if dataset == \"diabetes\":\n","        x, y = load_diabetes(return_X_y=True)\n","    elif dataset == \"boston\":\n","        x, y = load_boston(return_X_y=True)\n","    else:\n","        print('Invalid dataset')\n","        return\n","\n","    n = np.shape(x)[0]\n","    d = np.shape(x)[1]\n","\n","    # Standardize beforehand (for validity)\n","    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n","    y = (y - np.mean(y)) / np.std(y)\n","\n","    # Train test split\n","    ind_train, ind_test = train_test_split(np.arange(n),\n","                                           train_size=int(train_frac * n),\n","                                           random_state=seed)\n","    x_train = x[ind_train]\n","    y_train = y[ind_train]\n","    x_test = x[ind_test]\n","    y_test = y[ind_test]\n","\n","    y_plot = np.linspace(np.min(y_train) - 2, np.max(y_train) + 2, 100)\n","\n","    return x_train, y_train, x_test, y_test, y_plot, n, d\n","\n","\n","# #############################################################################\n","# BAYESIAN INFERENCE (MCMC)\n","# #############################################################################\n","\n","# Laplace prior PyMC3 model\n","def fit_mcmc_laplace(y, x, B, seed=100, misspec: bool = False):\n","    with pm.Model() as _:\n","        p = np.shape(x)[1]\n","        # Laplace\n","        b = pm.Gamma('b', alpha=1, beta=1)\n","        beta = pm.Laplace('beta', mu=0, b=b, shape=p)\n","        intercept = pm.Flat('intercept')\n","        if misspec:\n","            sigma = pm.HalfNormal(\"sigma\", sigma=0.02)  # misspec prior\n","        else:\n","            sigma = pm.HalfNormal(\"sigma\", sigma=1)\n","        obs = pm.Normal('obs', mu=pm.math.dot(x, beta) + intercept,\n","                        sigma=sigma, observed=y)\n","        trace = pm.sample(B, random_seed=seed, chains=4)\n","\n","    beta_post = trace['beta']\n","    intercept_post = trace['intercept'].reshape(-1, 1)\n","    sigma_post = trace['sigma'].reshape(-1, 1)\n","    b_post = trace['b'].reshape(-1, 1)\n","    print(np.mean(sigma_post))  # check misspec.\n","\n","    return beta_post, intercept_post, b_post, sigma_post\n","\n","\n","# #############################################################################\n","# APPLICATION OF THE BAYESIAN INFERENCE (MCMC)\n","\n","# Repeat 50 mcmc runs for different train test splits\n","def run_sparse_regression_mcmc(dataset, misspec: bool = False):\n","    # Repeat over 50 reps\n","    rep = 50\n","    train_frac = 0.7\n","    B = 2000\n","\n","    # Initialize\n","    x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_regression(\n","        train_frac, dataset, 100)\n","\n","    beta_post = np.zeros((rep, 4 * B, d))\n","    intercept_post = np.zeros((rep, 4 * B, 1))\n","    b_post = np.zeros((rep, 4 * B, 1))\n","    sigma_post = np.zeros((rep, 4 * B, 1))\n","    times = np.zeros(rep)\n","\n","    for j in tqdm(range(rep)):\n","        seed = 100 + j\n","        x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_regression(\n","            train_frac, dataset, seed)\n","        start = time.time()\n","        beta_post[j], intercept_post[j], b_post[j], sigma_post[\n","            j] = fit_mcmc_laplace(y, x, B, seed, misspec)\n","        end = time.time()\n","        times[j] = (end - start)\n","\n","    # Save posterior samples\n","    if misspec:\n","        suffix = dataset\n","    else:\n","        suffix = dataset + \"_misspec\"\n","\n","    print(\"{}: {} ({})\".format(suffix, np.mean(times),\n","                               np.std(times) / np.sqrt(rep)))\n","\n","    np.save(\"samples/beta_post_sparsereg_{}\".format(suffix), beta_post)\n","    np.save(\"samples/intercept_post_sparsereg_{}\".format(suffix),\n","            intercept_post)\n","    np.save(\"samples/b_post_sparsereg_{}\".format(suffix), b_post)\n","    np.save(\"samples/sigma_post_sparsereg_{}\".format(suffix), sigma_post)\n","    np.save(\"samples/times_sparsereg_{}\".format(suffix), times)\n","\n","\n","# #############################################################################\n","# CONFORMAL PREDICTION\n","# #############################################################################\n","\n","# Lasso split method baseline\n","def conformal_split(y, x, x_test, alpha, y_plot, seed=100):\n","    n = np.shape(y)[0]\n","    n_test = np.shape(x_test)[0]\n","    # Fit lasso to training set\n","    ls = LassoCV(cv=5, random_state=seed)\n","    n_train = int(n / 2)\n","    ls.fit(x[0:n_train], y[0:n_train])\n","    # Predict lasso on validation set\n","    y_pred_val = ls.predict(x[n_train:])\n","    resid = np.abs(y_pred_val - y[n_train:])\n","    k = int(np.ceil((n / 2 + 1) * (1 - alpha)))\n","    d = np.sort(resid)[k - 1]\n","    # Compute split conformal interval\n","    band_split = np.zeros((n_test, 2))\n","    y_pred_test = ls.predict(x_test)  # predict lasso on test\n","    band_split[:, 0] = y_pred_test - d\n","    band_split[:, 1] = y_pred_test + d\n","    return band_split\n","\n","\n","# Lasso full method baseline\n","def conformal_full(y, x, x_test, alpha, y_plot, C, seed=100):\n","    n = np.shape(y)[0]\n","    rank_full = np.zeros(np.shape(y_plot)[0])\n","    for i in range(np.shape(y_plot)[0]):\n","        y_new = y_plot[i]\n","        x_aug = np.concatenate((x, x_test), axis=0)\n","        y_aug = np.append(y, y_new)\n","        ls = Lasso(alpha=C, random_state=seed)\n","        ls.fit(x_aug, y_aug)\n","        y_pred_val = ls.predict(x_aug)\n","        resid = np.abs(y_pred_val - y_aug)\n","        rank_full[i] = np.sum(resid >= resid[-1]) / (n + 1)\n","    region_full = rank_full > alpha\n","    return region_full\n","\n","\n","# #############################################################################\n","# CONFORMAL BAYESIAN\n","# #############################################################################\n","\n","# CONFORMAL FROM MCMC SAMPLES (JAX IMPLEMENTATION)\n","\n","# Compute bayesian central 1-alpha credible interval from MCMC samples\n","@jit\n","def compute_bayes_band_MCMC(alpha, y_plot, cdf_pred):\n","    cdf_pred = jnp.mean(cdf_pred, axis=1)\n","\n","    band_bayes = np.zeros(2)\n","    band_bayes = index_update(band_bayes, 0, y_plot[\n","        jnp.argmin(jnp.abs(cdf_pred - alpha / 2))])\n","    band_bayes = index_update(band_bayes, 1, y_plot[\n","        jnp.argmin(jnp.abs(cdf_pred - (1 - alpha / 2)))])\n","    return band_bayes\n","\n","\n","# compute rank (un-normalized by n+1)\n","def compute_rank_IS(logp_samp_n, logwjk):\n","    # n = jnp.shape(logp_samp_n)[1]  # logp_samp_n is B x n\n","    # n_plot = jnp.shape(logwjk)[0]\n","    # rank_cp = jnp.zeros(n_plot)\n","\n","    # compute importance sampling weights and normalizing\n","    wjk = jnp.exp(logwjk)\n","    Zjk = jnp.sum(wjk, axis=1).reshape(-1, 1)\n","\n","    # compute predictives for y_i,x_i and y_new,x_n+1\n","    p_cp = jnp.dot(wjk / Zjk, jnp.exp(logp_samp_n))\n","    p_new = jnp.sum(wjk ** 2, axis=1).reshape(-1, 1) / Zjk\n","\n","    # compute nonconformity score and sort\n","    pred_tot = jnp.concatenate((p_cp, p_new), axis=1)\n","    rank_cp = np.sum(pred_tot <= pred_tot[:, -1].reshape(-1, 1), axis=1)\n","    return rank_cp\n","\n","\n","# compute region of grid which is in confidence set\n","@jit\n","def compute_cb_region_IS(alpha, logp_samp_n,\n","                         logwjk):  # assumes they are connected\n","    n = jnp.shape(logp_samp_n)[1]  # logp_samp_n is B x n\n","    rank_cp = compute_rank_IS(logp_samp_n, logwjk)\n","    region_true = rank_cp > alpha * (n + 1)\n","    return region_true\n","\n","\n","# #############################################################################\n","# MAIN PUBLIC FUNCTION (application of CONFORMAL BAYESIAN and the 3 baselines)\n","# #############################################################################\n","\n","def run_sparse_regression_conformal(dataset, misspec: bool = False):\n","    # Compute intervals\n","    # Initialize\n","    train_frac = 0.7\n","    x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_regression(\n","        train_frac, dataset, 100)\n","\n","    # Load posterior samples\n","    if misspec:\n","        suffix = dataset\n","    else:\n","        suffix = dataset + \"_misspec\"\n","\n","    beta_post = jnp.load(\"samples/beta_post_sparsereg_{}.npy\".format(suffix))\n","    intercept_post = jnp.load(\n","        \"samples/intercept_post_sparsereg_{}.npy\".format(suffix))\n","    sigma_post = jnp.load(\"samples/sigma_post_sparsereg_{}.npy\".format(suffix))\n","\n","    # Initialize\n","    alpha = 0.2\n","    rep = np.shape(beta_post)[0]\n","    n_test = np.shape(x_test)[0]\n","\n","    coverage_cb = np.zeros((rep, n_test))\n","    coverage_cb_exact = np.zeros((rep, n_test))  # avoiding grid effects\n","    coverage_bayes = np.zeros((rep, n_test))\n","    coverage_split = np.zeros((rep, n_test))\n","    coverage_full = np.zeros((rep, n_test))\n","\n","    length_cb = np.zeros((rep, n_test))\n","    length_bayes = np.zeros((rep, n_test))\n","    length_split = np.zeros((rep, n_test))\n","    length_full = np.zeros((rep, n_test))\n","\n","    band_bayes = np.zeros((rep, n_test, 2))\n","    region_cb = np.zeros((rep, n_test, np.shape(y_plot)[0]))\n","    region_full = np.zeros((rep, n_test, np.shape(y_plot)[0]))\n","    band_split = np.zeros((rep, n_test, 2))\n","\n","    times_bayes = np.zeros(rep)\n","    times_cb = np.zeros(rep)\n","    times_split = np.zeros(rep)\n","    times_full = np.zeros(rep)\n","\n","    for j in tqdm(range(rep)):\n","        seed = 100 + j\n","        # load dataset\n","        x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_regression(\n","            train_frac, dataset, seed)\n","        dy = y_plot[1] - y_plot[0]\n","\n","        # split method\n","        start = time.time()\n","        band_split[j] = conformal_split(y, x, x_test, alpha, y_plot, seed)\n","        coverage_split[j] = (y_test >= band_split[j, :, 0]) & (\n","                    y_test <= band_split[j, :, 1])\n","        length_split[j] = np.abs(band_split[j, :, 0] - band_split[j, :, 1])\n","        end = time.time()\n","        times_split[j] = end - start\n","\n","        # full method\n","        start = time.time()\n","        C = 0.004\n","        for i in (range(n_test)):\n","            region_full[j, i] = conformal_full(y, x, x_test[i:i + 1], alpha,\n","                                               y_plot, C, seed)\n","            coverage_full[j, i] = region_full[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]\n","            length_full[j, i] = np.sum(region_full[j, i]) * dy\n","        end = time.time()\n","        times_full[j] = end - start\n","\n","        # Bayes\n","        start = time.time()\n","\n","        @jit  # normal cdf from posterior samples\n","        def normal_likelihood_cdf(_y, _x):\n","            return norm.cdf(\n","                _y, loc=jnp.dot(beta_post[j],\n","                                _x.transpose()) + intercept_post[j],\n","                scale=sigma_post[j])  # compute likelihood samples\n","\n","        # Precompute cdfs\n","        cdf_test = normal_likelihood_cdf(y_plot.reshape(-1, 1, 1), x_test)\n","\n","        for i in (range(n_test)):\n","            band_bayes[j, i] = compute_bayes_band_MCMC(\n","                alpha, y_plot, cdf_test[:, :, i])\n","            coverage_bayes[j, i] = (y_test[i] >= band_bayes[j, i, 0]) & (\n","                        y_test[i] <= band_bayes[j, i, 1])\n","            length_bayes[j, i] = np.abs(\n","                band_bayes[j, i, 1] - band_bayes[j, i, 0])\n","        end = time.time()\n","        times_bayes[j] = end - start\n","\n","        # Conformal Bayes\n","        start = time.time()\n","\n","        @jit  # normal loglik from posterior samples\n","        def normal_loglikelihood(_y, _x):\n","            return norm.logpdf(\n","                _y,\n","                loc=jnp.dot(beta_post[j], _x.transpose()) + intercept_post[j],\n","                scale=sigma_post[j])  # compute likelihood samples\n","\n","        logp_samp_n = normal_loglikelihood(y, x)\n","        logwjk = normal_loglikelihood(y_plot.reshape(-1, 1, 1), x_test)\n","        logwjk_test = normal_loglikelihood(y_test, x_test).reshape(1, -1,\n","                                                                   n_test)\n","\n","        for i in (range(n_test)):\n","            region_cb[j, i] = compute_cb_region_IS(\n","                alpha, logp_samp_n, logwjk[:, :, i])\n","            coverage_cb[j, i] = region_cb[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]  # grid coverage\n","            length_cb[j, i] = np.sum(region_cb[j, i]) * dy\n","        end = time.time()\n","        times_cb[j] = end - start\n","\n","        # compute exact coverage to avoid grid effects\n","        for i in (range(n_test)):\n","            coverage_cb_exact[j, i] = compute_cb_region_IS(\n","                alpha, logp_samp_n, logwjk_test[:, :, i])  # exact coverage\n","\n","    # #Save regions (need to update)\n","    np.save(\"results/region_cb_sparsereg_{}\".format(suffix), region_cb)\n","    np.save(\"results/band_bayes_sparsereg_{}\".format(suffix), band_bayes)\n","    np.save(\"results/band_split_sparsereg_{}\".format(suffix), band_split)\n","    np.save(\"results/region_full_sparsereg_{}\".format(suffix), band_split)\n","\n","    np.save(\"results/coverage_cb_sparsereg_{}\".format(suffix), coverage_cb)\n","    np.save(\"results/coverage_cb_exact_sparsereg_{}\".format(suffix),\n","            coverage_cb_exact)\n","    np.save(\"results/coverage_bayes_sparsereg_{}\".format(suffix),\n","            coverage_bayes)\n","    np.save(\"results/coverage_split_sparsereg_{}\".format(suffix),\n","            coverage_split)\n","    np.save(\"results/coverage_full_sparsereg_{}\".format(suffix), coverage_full)\n","\n","    np.save(\"results/length_cb_sparsereg_{}\".format(suffix), length_cb)\n","    np.save(\"results/length_bayes_sparsereg_{}\".format(suffix), length_bayes)\n","    np.save(\"results/length_split_sparsereg_{}\".format(suffix), length_split)\n","    np.save(\"results/length_full_sparsereg_{}\".format(suffix), length_full)\n","\n","    np.save(\"results/times_cb_sparsereg_{}\".format(suffix), times_cb)\n","    np.save(\"results/times_bayes_sparsereg_{}\".format(suffix), times_bayes)\n","    np.save(\"results/times_split_sparsereg_{}\".format(suffix), times_split)\n","    np.save(\"results/times_full_sparsereg_{}\".format(suffix), times_full)"],"metadata":{"id":"VqwcHmanCvKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Running the scripts"],"metadata":{"id":"C_EburtvDzar"}},{"cell_type":"code","source":["makedirs('samples', exist_ok=True)\n","makedirs('results', exist_ok=True)\n","\n","# run MCMC\n","run_sparse_regression_mcmc('diabetes', misspec=False)  # poorly-specified\n","run_sparse_regression_mcmc('diabetes', misspec=True)  # well-specified\n","\n","# run Conformal Bayes\n","run_sparse_regression_conformal('diabetes', misspec=False)  # poorly-specified\n","run_sparse_regression_conformal('diabetes', misspec=True)  # well-specified"],"metadata":{"id":"O9EGEQzCDa7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 Sparse Classification"],"metadata":{"id":"CQ-9Ml5QkI5V"}},{"cell_type":"markdown","source":["The authors apply the CBC framework to sparse classification with the [`sklearn` Wisconsin breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) (Wolberg and Mangasarian, 1990): a regression when the number of predictors (cell nuclei measurements as covariates) is large ($d=30$) and there  are few samples ($n=569$).\n","\n","For reproducibility, all the predictors were normalized to have mean $0$ and standard deviation $1$ (the response variable here is binary, corresponding to malignant or benignant tumour). We will compute the Bayesian predictive set, which is the smallest set from $\\{0\\},\\{1\\},\\{0,1\\}$ that contains at least $(1-\\alpha)$ of the posterior predictive probability.\n","\n","Here, we consider the logistic likelihood $f_\\theta(y=1 \\mid x)=\\left[1+\\exp \\left\\{-\\left(\\theta^{\\mathrm{T}} x+\\theta_0\\right)\\right\\}\\right]^{-1}$, with the same priors for $\\theta, \\theta_0$ as in the **sparse regression** case."],"metadata":{"id":"3VhThv9vEhch"}},{"cell_type":"markdown","source":["### Code"],"metadata":{"id":"_WF1QJfREJ9C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4PGz2LanBR_"},"outputs":[],"source":["\"\"\"\n","Summary of the Fong & Holmes (2021) implementation on Conformal Bayesian\n","Computation. Specifically, this module deals with the sparse classification\n","(and its uncertainty quantification) assessed for the:\n","- UCI ML Breast Cancer Wisconsin dataset (breast)\n","\n","The following 4 methods are implemented:\n","- Bayesian inference (bayes)\n","- Conformal bayes (cb)\n","- Split conformal prediction (split)\n","- Full conformal prediction (full)\n","\n","A brief summary of the models can be found below:\n","- 'bayes': This method uses the posterior samples of the model parameters to\n","compute the likelihood of the test point belonging to each class. It uses the\n","likelihoods to construct a prediction interval.\n","- 'split': This method fits a LASSO logistic regression model to the first\n","half of the training data, then computes the residuals on the second half\n","of the training data. It uses the residuals to define a threshold for the\n","prediction intervals, which are constructed using the logistic regression\n","model on the testing data.\n","- 'full': This method fits a logistic regression model to the combined\n","training and testing data, then computes the rank of the test point in\n","the combined data set. It uses the rank to define a threshold for the\n","prediction intervals, which are constructed using the logistic\n","regression model on the test point.\n","- 'cb': uses the posterior distribution from MCMC sampling to define\n","conformal prediction intervals.\n","\n","\n","The execution chain is as it follows:\n","- The data is loaded using `load_train_test_sparse_classification`\n","- The Bayesian inference (bayes) is performed through the function\n","`run_sparse_classification_mcmc` (which requires the MCMC computations\n","defined at `fit_mcmc_laplace`).\n","- Then, using the former results, the Conformal Bayesian method\n","(along the 2 other baselines) is applied using the function\n","`run_sparse_classification_conformal`:\n","  - The 'split' and 'full' conformal baselines are defined in the functions\n","`conformal_split` and `conformal_full`, respectively.\n","  - The 'cb' method is implemented in the function `compute_cb_region_IS`\n","  and it also uses the own `logistic_loglikelihood` clause.\n","- Once launched the main function `run_sparse_classification_conformal`, it\n","loads the training and testing data and the posterior samples of the model\n","parameters (computed because `run_sparse_classification_mcmc` was run first).\n","- Then it iterates through a specified number of repetitions, applying each\n","of the four methods to compute prediction intervals and coverage probabilities\n","for each test point.\n","- For each repetition, the function applies the split method, the full method,\n","the Bayesian method, and the conformal Bayes method to compute the prediction\n","intervals and coverage probabilities. It also records the computation time\n","for each method.\n","- At the end of the function, these results are saved to various files,\n","including the coverage probabilities, the lengths of the prediction\n","intervals, and the computation times for each method.\n","\n","\"\"\"\n","\n","\n","# #############################################################################\n","# LOAD DATA (sparse classification)\n","# #############################################################################\n","\n","def load_train_test_sparse_classification(train_frac, dataset, seed):\n","    # Load dataset\n","    if dataset == \"breast\":\n","        x, y = load_breast_cancer(return_X_y=True)\n","    elif dataset == \"parkinsons\":\n","        data = pd.read_csv('data/parkinsons.data')\n","        data[data == '?'] = np.nan\n","        data.dropna(axis=0, inplace=True)\n","        y = data['status'].values  # convert strings to integer\n","        x = data.drop(columns=['name', 'status']).values\n","    else:\n","        print('Invalid dataset')\n","        return\n","\n","    n = np.shape(x)[0]\n","    d = np.shape(x)[1]\n","\n","    # Standardize beforehand (for validity)\n","    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n","\n","    # Train test split\n","    ind_train, ind_test = train_test_split(np.arange(n),\n","                                           train_size=int(train_frac * n),\n","                                           random_state=seed)\n","    x_train = x[ind_train]\n","    y_train = y[ind_train]\n","    x_test = x[ind_test]\n","    y_test = y[ind_test]\n","\n","    y_plot = np.array([0, 1])\n","\n","    return x_train, y_train, x_test, y_test, y_plot, n, d\n","\n","\n","# #############################################################################\n","# BAYESIAN INFERENCE (MCMC)\n","# #############################################################################\n","\n","# Laplace prior PyMC3 model\n","def fit_mcmc_laplace(y, x, B, seed=100):\n","    with pm.Model() as _:  # as model\n","        p = np.shape(x)[1]\n","        # Laplace\n","        b = pm.Gamma('b', alpha=1, beta=1)\n","        beta = pm.Laplace('beta', mu=0, b=b, shape=p)\n","        intercept = pm.Flat('intercept')\n","        obs = pm.Bernoulli(\n","            'obs', logit_p=pm.math.dot(x, beta) + intercept, observed=y)\n","        trace = pm.sample(B, random_seed=seed, chains=4)\n","\n","    beta_post = trace['beta']\n","    intercept_post = trace['intercept'].reshape(-1, 1)\n","    b_post = trace['b'].reshape(-1, 1)\n","\n","    return beta_post, intercept_post, b_post\n","\n","\n","# #############################################################################\n","# APPLICATION OF THE BAYESIAN INFERENCE (MCMC)\n","\n","# repeat 50 mcmc runs for different train test splits\n","def run_sparse_classification_mcmc(dataset):\n","    # Repeat over 50 reps\n","    rep = 50\n","    train_frac = 0.7\n","    B = 2000\n","\n","    # Initialize\n","    x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_classification(\n","        train_frac, dataset, 100)\n","\n","    beta_post = np.zeros((rep, 4 * B, d))\n","    intercept_post = np.zeros((rep, 4 * B, 1))\n","    b_post = np.zeros((rep, 4 * B, 1))\n","    times = np.zeros(rep)\n","\n","    for j in tqdm(range(rep)):\n","        seed = 100 + j\n","        x, y, x_test, y_test, y_plot, n, d = \\\n","            load_train_test_sparse_classification(train_frac, dataset, seed)\n","        start = time.time()\n","        beta_post[j], intercept_post[j], b_post[j] = fit_mcmc_laplace(y, x, B,\n","                                                                      seed)\n","        end = time.time()\n","        times[j] = (end - start)\n","\n","    print(\"{}: {} ({})\".format(dataset, np.mean(times),\n","                               np.std(times) / np.sqrt(rep)))\n","\n","    # Save posterior samples\n","    np.save(\"samples/beta_post_sparseclass_{}\".format(dataset), beta_post)\n","    np.save(\"samples/intercept_post_sparseclass_{}\".format(dataset),\n","            intercept_post)\n","    np.save(\"samples/b_post_sparseclass_{}\".format(dataset), b_post)\n","    np.save(\"samples/times_sparseclass_{}\".format(dataset), times)\n","\n","\n","# #############################################################################\n","# CONFORMAL PREDICTION\n","# #############################################################################\n","\n","# Split method baseline\n","def conformal_split(alpha, y, x, x_test, seed=100):\n","    n = np.shape(y)[0]\n","    #  n_test = np.shape(x_test)[0]\n","    # Fit lasso to training set\n","    n_train = int(n / 2)\n","    ls = LogisticRegressionCV(penalty='l1', solver='liblinear', cv=5,\n","                              random_state=seed)\n","    ls.fit(x[0:n_train], y[0:n_train])\n","    resid = ls.predict_proba(x[n_train:])[:, 1]\n","    resid[y[n_train:] == 0] = 1 - resid[y[n_train:] == 0]\n","    resid = -np.log(\n","        np.clip(resid, 1e-6, 1 - 1e-6))  # clip for numerical stability\n","    k = int(np.ceil((n / 2 + 1) * (1 - alpha)))\n","    d = np.sort(resid)[k - 1]\n","\n","    logp_test = -np.log(np.clip(ls.predict_proba(x_test), 1e-6, 1 - 1e-6))\n","    region_split = logp_test <= d\n","\n","    return region_split\n","\n","\n","# Full method baseline\n","def conformal_full(alpha, y, x, x_test, C, seed=100):\n","    n = np.shape(y)[0]\n","    rank_cp = np.zeros(2)\n","    for y_new in (0, 1):\n","        x_aug = np.concatenate((x, x_test), axis=0)\n","        y_aug = np.append(y, y_new)\n","        ls = LogisticRegression(penalty='l1', solver='liblinear', C=C,\n","                                random_state=seed)\n","        ls.fit(x_aug, y_aug)\n","        resid = ls.predict_proba(x_aug)[:, 1]\n","        resid[y_aug == 0] = 1 - resid[y_aug == 0]\n","        resid = -np.log(resid)\n","        rank_cp[y_new] = np.sum(resid >= resid[-1]) / (n + 1)\n","    region_full = rank_cp > alpha\n","    return region_full\n","\n","\n","# #############################################################################\n","# CONFORMAL BAYESIAN\n","# #############################################################################\n","\n","# CONFORMAL FROM MCMC SAMPLES (JAX IMPLEMENTATION)\n","\n","# compute rank (un-normalized by n+1)\n","\n","@jit\n","def compute_rank_IS(logp_samp_n, logwjk):\n","    # n = jnp.shape(logp_samp_n)[1]  # logp_samp_n is B x n\n","    # n_plot = jnp.shape(logwjk)[0]\n","    # rank_cp = jnp.zeros(n_plot)\n","\n","    # compute importance sampling weights and normalizing\n","    wjk = jnp.exp(logwjk)\n","    Zjk = jnp.sum(wjk, axis=1).reshape(-1, 1)\n","\n","    # compute predictives for y_i,x_i and y_new,x_n+1\n","    p_cp = jnp.dot(wjk / Zjk, jnp.exp(logp_samp_n))\n","    p_new = jnp.sum(wjk ** 2, axis=1).reshape(-1, 1) / Zjk\n","\n","    # compute non-conformity score and sort\n","    pred_tot = jnp.concatenate((p_cp, p_new), axis=1)\n","    rank_cp = np.sum(pred_tot <= pred_tot[:, -1].reshape(-1, 1), axis=1)\n","    return rank_cp\n","\n","\n","# compute region of grid which is in confidence set\n","@jit\n","def compute_cb_region_IS(alpha, logp_samp_n,\n","                         logwjk):  # assumes they are connected\n","    n = jnp.shape(logp_samp_n)[1]  # logp_samp_n is B x n\n","    rank_cp = compute_rank_IS(logp_samp_n, logwjk)\n","    region_true = rank_cp > alpha * (n + 1)\n","    return region_true\n","\n","\n","# #############################################################################\n","# MAIN PUBLIC FUNCTION (application of CONFORMAL BAYESIAN and the 3 baselines)\n","# #############################################################################\n","\n","def run_sparse_classification_conformal(dataset):\n","    # Compute intervals\n","    # Load posterior samples\n","    beta_post = jnp.load(\n","        \"samples/beta_post_sparseclass_{}.npy\".format(dataset))\n","    intercept_post = jnp.load(\n","        \"samples/intercept_post_sparseclass_{}.npy\".format(dataset))\n","\n","    # Initialize\n","    train_frac = 0.7\n","    x, y, x_test, y_test, y_plot, n, d = load_train_test_sparse_classification(\n","        train_frac, dataset, 100)\n","\n","    alpha = 0.2\n","    rep = np.shape(beta_post)[0]\n","    n_test = np.shape(x_test)[0]\n","\n","    coverage_cb = np.zeros((rep, n_test))\n","    coverage_bayes = np.zeros((rep, n_test))\n","    coverage_split = np.zeros((rep, n_test))\n","    coverage_full = np.zeros((rep, n_test))\n","\n","    length_cb = np.zeros((rep, n_test))\n","    length_bayes = np.zeros((rep, n_test))\n","    length_split = np.zeros((rep, n_test))\n","    length_full = np.zeros((rep, n_test))\n","\n","    p_bayes = np.zeros((rep, n_test))\n","    region_bayes = np.zeros((rep, n_test, 2))\n","    region_cb = np.zeros((rep, n_test, 2))\n","    region_split = np.zeros((rep, n_test, 2))\n","    region_full = np.zeros((rep, n_test, 2))\n","\n","    times_bayes = np.zeros(rep)\n","    times_cb = np.zeros(rep)\n","    times_split = np.zeros(rep)\n","    times_full = np.zeros(rep)\n","\n","    for j in tqdm(range(rep)):\n","        seed = 100 + j\n","\n","        # load data\n","        x, y, x_test, y_test, y_plot, n, d = \\\n","            load_train_test_sparse_classification(train_frac, dataset, seed)\n","\n","        # Split conformal method\n","        start = time.time()\n","        region_split[j] = conformal_split(alpha, y, x, x_test, seed)\n","        for i in (range(n_test)):\n","            coverage_split[j, i] = region_split[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]\n","            length_split[j, i] = np.sum(region_split[j, i])\n","        end = time.time()\n","        times_split[j] = end - start\n","\n","        # Full conformal method\n","        start = time.time()\n","        C = 1.\n","        for i in (range(n_test)):\n","            region_full[j, i] = conformal_full(alpha, y, x, x_test[i:i + 1], C,\n","                                               seed)\n","            coverage_full[j, i] = region_full[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]\n","            length_full[j, i] = np.sum(region_full[j, i])\n","        end = time.time()\n","        times_full[j] = end - start\n","\n","        # ###########################\n","        @jit\n","        def logistic_loglikelihood(_y, _x):\n","            eta = (jnp.dot(beta_post[j], _x.transpose()) + intercept_post[j])\n","            B = np.shape(eta)[0]\n","            _n = np.shape(eta)[1]\n","            eta = eta.reshape(B, _n, 1)\n","            temp0 = np.zeros((B, _n, 1))\n","            logp = -jsp.special.logsumexp(\n","                jnp.concatenate((temp0, -eta), axis=2),\n","                axis=2)  # numerically stable\n","            log1p = -jsp.special.logsumexp(\n","                jnp.concatenate((temp0, eta), axis=2), axis=2)\n","            return _y * logp + (1 - _y) * log1p  # compute likelihood samples\n","\n","        # ###########################\n","\n","        # Bayes\n","        start = time.time()\n","        for i in (range(n_test)):\n","            p_bayes[j, i] = jnp.mean(\n","                jnp.exp(logistic_loglikelihood(1, x_test[i:i + 1])))\n","            # Compute region from p_bayes\n","            if p_bayes[j, i] > (1 - alpha):  # only y = 1\n","                region_bayes[j, i] = np.array([0, 1])\n","            elif (1 - p_bayes[j, i]) > (1 - alpha):  # only y = 0\n","                region_bayes[j, i] = np.array([1, 0])\n","            else:\n","                region_bayes[j, i] = np.array([1, 1])\n","            coverage_bayes[j, i] = region_bayes[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]\n","            length_bayes[j, i] = np.sum(region_bayes[j, i])\n","        end = time.time()\n","        times_bayes[j] = end - start\n","\n","        # Conformal Bayes\n","        start = time.time()\n","        logp_samp_n = logistic_loglikelihood(y, x)\n","        logwjk = logistic_loglikelihood(y_plot.reshape(-1, 1, 1), x_test)\n","        for i in (range(n_test)):\n","            region_cb[j, i] = compute_cb_region_IS(\n","                alpha, logp_samp_n, logwjk[:, :, i])\n","            coverage_cb[j, i] = region_cb[\n","                j, i, np.argmin(np.abs(y_test[i] - y_plot))]\n","            length_cb[j, i] = np.sum(region_cb[j, i])\n","        end = time.time()\n","        times_cb[j] = end - start\n","\n","    # Save regions (need to update)\n","    np.save(\"results/p_bayes_sparseclass_{}\".format(dataset), p_bayes)\n","    np.save(\"results/region_bayes_sparseclass_{}\".format(dataset),\n","            region_bayes)\n","    np.save(\"results/region_cb_sparseclass_{}\".format(dataset), region_cb)\n","    np.save(\"results/region_split_sparseclass_{}\".format(dataset),\n","            region_split)\n","    np.save(\"results/region_full_sparseclass_{}\".format(dataset), region_full)\n","\n","    np.save(\"results/coverage_bayes_sparseclass_{}\".format(dataset),\n","            coverage_bayes)\n","    np.save(\"results/coverage_cb_sparseclass_{}\".format(dataset), coverage_cb)\n","    np.save(\"results/coverage_split_sparseclass_{}\".format(dataset),\n","            coverage_split)\n","    np.save(\"results/coverage_full_sparseclass_{}\".format(dataset),\n","            coverage_full)\n","\n","    np.save(\"results/length_bayes_sparseclass_{}\".format(dataset),\n","            length_bayes)\n","    np.save(\"results/length_cb_sparseclass_{}\".format(dataset), length_cb)\n","    np.save(\"results/length_split_sparseclass_{}\".format(dataset),\n","            length_split)\n","    np.save(\"results/length_full_sparseclass_{}\".format(dataset), length_full)\n","\n","    np.save(\"results/times_bayes_sparseclass_{}\".format(dataset), times_bayes)\n","    np.save(\"results/times_cb_sparseclass_{}\".format(dataset), times_cb)\n","    np.save(\"results/times_split_sparseclass_{}\".format(dataset), times_split)\n","    np.save(\"results/times_full_sparseclass_{}\".format(dataset), times_full)"]},{"cell_type":"markdown","source":["### Runnning the scripts"],"metadata":{"id":"nvUID39TEKdq"}},{"cell_type":"code","source":["os.makedirs('samples', exist_ok=True)\n","os.makedirs('results', exist_ok=True)\n","run_sparse_classification_mcmc('breast')\n","run_sparse_classification_conformal('breast')"],"metadata":{"id":"vsdoLA5WEK2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"9aKySzFgD08b"}},{"cell_type":"markdown","source":["Firstly, the results of the former scripts must be placed at the `results` folder. If the Google Collab was not run (due to `conda` issues or time limits), then the files which were obtained locally can be fetched from the [results.zip](https://github.com/gcastro-98/conformal-bayesian/blob/main/results.zip).\n","\n","Below the main scripts to retrieve the results are presented:"],"metadata":{"id":"7ubKSLPpPwTI"}},{"cell_type":"code","source":["EXAMPLES = [\n","    'sparsereg_diabetes', 'sparsereg_diabetes_misspec', 'sparseclass_breast'\n","]\n","METHODS = ['bayes', 'cb', 'split', 'full']\n","\n","\n","# def report_mcmc_times() -> None:\n","#     \"\"\"\n","#     Reports how much time was elapsed in the\n","#     sampling for each posterior distribution.\n","#     \"\"\"\n","#     for example in EXAMPLES:\n","#         suffix = example\n","#         times = np.load(\"samples/times_{}.npy\".format(suffix))\n","#         rep = np.shape(times)[0]\n","#         print(\"{} MCMC time: {:.3f} ({:.3f})\".format(\n","#             suffix, np.mean(times), np.std(times)/np.sqrt(rep)))\n","#     print()\n","\n","\n","# ############################################################################\n","# MAIN PUBLIC FUNCTION\n","# ############################################################################\n","\n","def report_results(regression: bool) -> None:\n","    if regression:\n","        _EXAMPLES = EXAMPLES[:2]\n","    else:\n","        _EXAMPLES = [EXAMPLES[2]]\n","\n","    for example in _EXAMPLES:\n","        print('EXAMPLE: {}'.format(example))\n","        for method in METHODS:\n","            suffix = method + '_' + example\n","\n","            # Coverage (take mean over test values)\n","\n","            coverage = np.mean(np.load(\n","                \"results/coverage_{}.npy\".format(suffix)), axis=1)\n","            rep = np.shape(coverage)[0]\n","            mean = np.mean(coverage)\n","            se = np.std(coverage)/np.sqrt(rep)\n","            print(\"{} coverage is {:.3f} ({:.3f})\".format(\n","                method, mean, se))\n","\n","            # Return exact coverage if cb\n","            if method == 'cb' and regression:\n","                suffix_ex = method + '_exact_' + example\n","                coverage = np.mean(np.load(\"results/coverage_{}.npy\".format(\n","                    suffix_ex)), axis=1)  # take mean over test values\n","                rep = np.shape(coverage)[0]\n","                mean = np.mean(coverage)\n","                se = np.std(coverage)/np.sqrt(rep)\n","                print(\"{} exact coverage is {:.3f} ({:.3f})\".format(\n","                    method, mean, se))\n","        print()\n","\n","        for method in METHODS:\n","            suffix = method + '_' + example\n","            # Length\n","            length = np.mean(np.load(\n","                \"results/length_{}.npy\".format(suffix)), axis=1)\n","            rep = np.shape(length)[0]\n","            mean = np.mean(length)\n","            se = np.std(length)/np.sqrt(rep)\n","            print(\"{} length is {:.2f} ({:.2f})\".format(method, mean, se))\n","        print()\n","\n","        for method in METHODS:\n","            suffix = method + '_' + example\n","            # Times\n","            times = np.load(\"results/times_{}.npy\".format(suffix))\n","            rep = np.shape(times)[0]\n","            mean = np.mean(times)\n","            se = np.std(times)/np.sqrt(rep)\n","            print(\"{} times is {:.3f} ({:.3f})\".format(method, mean, se))\n","        print()\n","\n","\n","# ############################################################################\n","# Sparse classification (only) routines\n","# ############################################################################\n","\n","def report_missclassification_rates() -> None:\n","    example = 'sparseclass_breast'\n","    for method in ['bayes', 'cb']:\n","        suffix = method + '_' + example\n","        coverage = np.load(\"results/coverage_{}.npy\".format(suffix))\n","        length = np.load(\"results/length_{}.npy\".format(suffix))\n","        rep = np.shape(coverage)[0]\n","        n_tot = np.sum(length == 1, axis=1)\n","        n_misclass = np.sum(\n","            np.logical_and(length == 1, coverage == 0), axis=1)\n","        misclass_rate = n_misclass/n_tot\n","        both_rate = np.mean(length == 2, axis=1)\n","        empty_rate = np.mean(length == 0, axis=1)\n","\n","        print('{} misclassification rate is {:.3f} ({:.3f})'.format(\n","            method, np.mean(misclass_rate),\n","            np.std(misclass_rate)/np.sqrt(rep)))\n","        print('{} both rate is {:.3f} ({:.3f})'.format(\n","            method, np.mean(both_rate),\n","            np.std(both_rate)/np.sqrt(rep)))\n","        print('{} empty rate is {:.3f} ({:.3f})'.format(\n","            method, np.mean(empty_rate),\n","            np.std(empty_rate)/np.sqrt(rep)))"],"metadata":{"id":"cqsDn6MDD6vi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.1 Sparse regression"],"metadata":{"id":"dJdD3y3wD3rD"}},{"cell_type":"markdown","source":["As baselines, we compare to the split and full conformal method using the non-Bayesian lasso as the predictor, with the usual residual as the  nonconformity score. For the split method, we fit lasso with cross-validation on the subset of size $n_{\\mathrm{train}}/2$ to obtain the lasso penalty $\\lambda$. For the full conformal method, we use the grid method for fair timing, as other estimators beyond lasso would not have the shortcut of Lei (2019). As setting a default $\\lambda = 1$ gives poor average lengths, we estimate $λ = 0.004$ in cross-validation on one of the training sets, and use this value over the 50 repeats."],"metadata":{"id":"Dub854l2D0uD"}},{"cell_type":"code","source":["report_results(regression=True)"],"metadata":{"id":"jI1vOyvqD5nF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687272500554,"user_tz":-120,"elapsed":232,"user":{"displayName":"Gerard Castro","userId":"08621642096776231624"}},"outputId":"078b881f-718c-470e-ce79-cb2fe9751582"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EXAMPLE: sparsereg_diabetes\n","bayes coverage is 0.806 (0.005)\n","cb coverage is 0.808 (0.006)\n","cb exact coverage is 0.810 (0.005)\n","split coverage is 0.809 (0.006)\n","full coverage is 0.808 (0.006)\n","\n","bayes length is 1.84 (0.01)\n","cb length is 1.87 (0.01)\n","split length is 1.91 (0.02)\n","full length is 1.86 (0.01)\n","\n","bayes times is 0.488 (0.107)\n","cb times is 0.702 (0.019)\n","split times is 0.065 (0.001)\n","full times is 11.529 (0.232)\n","\n","EXAMPLE: sparsereg_diabetes_misspec\n","bayes coverage is 0.563 (0.006)\n","cb coverage is 0.809 (0.006)\n","cb exact coverage is 0.810 (0.006)\n","split coverage is 0.809 (0.006)\n","full coverage is 0.808 (0.006)\n","\n","bayes length is 1.14 (0.00)\n","cb length is 1.87 (0.01)\n","split length is 1.91 (0.02)\n","full length is 1.86 (0.01)\n","\n","bayes times is 0.373 (0.002)\n","cb times is 0.668 (0.003)\n","split times is 0.066 (0.001)\n","full times is 11.524 (0.240)\n","\n"]}]},{"cell_type":"markdown","source":["The average coverage, length and run-times with standard errors are given above for $\\alpha = 0.2$ ($80\\%$ of confidence). Note that:\n","- For $c=1$ (well-specified), the Bayesian intervals have coverage close to $1 − \\alpha$ with the smallest expected length, with CB slightly wider and more conservative.\n","- For $c=0.02$ (when the prior is misspecified), the Bayes intervals severely undercover, whilst the CB coverage and length remain unchanged from the c = 1 case.\n","- The split method has wider intervals than CB/full, but performs well given the low computational costs (of course in the split/full methods, the values of $c$ do not change anything).\n","- The full conformal method performs as well as CB, but is comparable in time as\n","MCMC + CB, whilst not refitting $\\lambda$."],"metadata":{"id":"32ipWU2SGJ-V"}},{"cell_type":"markdown","source":["## 4.2. Sparse classification"],"metadata":{"id":"_pTYNrw4GJ79"}},{"cell_type":"markdown","source":["The conformal baselines are as above but with $L_1$-penalized logistic regression and for the full conformal method we have $\\lambda = 1$. We again have 50 repeats with 70-30 train-test split, and set $\\alpha = 0.2$."],"metadata":{"id":"cxm3dGZyGJxV"}},{"cell_type":"code","source":["report_results(regression=False)"],"metadata":{"id":"GcpBqh3EGNDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687276811769,"user_tz":-120,"elapsed":241,"user":{"displayName":"Gerard Castro","userId":"08621642096776231624"}},"outputId":"8b893081-0558-424a-b0f1-7dda195e8e3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EXAMPLE: sparseclass_breast\n","bayes coverage is 0.990 (0.001)\n","cb coverage is 0.812 (0.005)\n","split coverage is 0.809 (0.006)\n","full coverage is 0.811 (0.005)\n","\n","bayes length is 1.06 (0.00)\n","cb length is 0.81 (0.00)\n","split length is 0.81 (0.01)\n","full length is 0.81 (0.00)\n","\n","bayes times is 0.364 (0.007)\n","cb times is 0.665 (0.012)\n","split times is 0.079 (0.002)\n","full times is 1.008 (0.016)\n","\n"]}]},{"cell_type":"markdown","source":["The results are provided above. It is easy to note that Bayes over-covers substantially, even with reasonable priors. However, CB corrects it spending little more time on it.\n","\n","Moreover, the misclassification rates for the Bayes and the CB method are presented below, being the latter the lowest one."],"metadata":{"id":"IZPZLk8KGqj0"}},{"cell_type":"code","source":["report_missclassification_rates()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aLIEsNXh8dC","executionInfo":{"status":"ok","timestamp":1687276817146,"user_tz":-120,"elapsed":242,"user":{"displayName":"Gerard Castro","userId":"08621642096776231624"}},"outputId":"cfacf67e-74c1-4cd9-81ae-9de56a8c1105"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bayes misclassification rate is 0.011 (0.001)\n","bayes both rate is 0.059 (0.002)\n","bayes empty rate is 0.000 (0.000)\n","cb misclassification rate is 0.002 (0.000)\n","cb both rate is 0.000 (0.000)\n","cb empty rate is 0.186 (0.005)\n"]}]},{"cell_type":"markdown","source":["# Conclusions"],"metadata":{"id":"jR0j9JeIh87a"}},{"cell_type":"markdown","source":["In conclusion, the Conformal Bayesian (CB) method offers significant advantages over traditional Bayesian inference approaches. One notable advantage is its superior coverage performance. The CB method constructs prediction regions that have a specified coverage probability, providing valid measures of confidence for individual predictions. This ensures that the true value falls within the predicted region with a high degree of accuracy, resulting in reliable uncertainty estimates.\n","\n","Furthermore, the CB method demonstrates robustness in the presence of misspecified priors. In Bayesian inference, the choice of prior distributions can have a substantial impact on the posterior estimates. If the priors do not accurately represent the true underlying distribution, the posterior estimates may be biased or unreliable. In contrast, the CB method's construction of prediction regions is not as heavily dependent on the choice of priors. It focuses on achieving the desired coverage probability based on the observed data, making it more robust to misspecification of the prior distribution.\n","\n","Additionally, the CB method benefits from the computational efficiency provided by the AOI (Add-One-In) sampling procedure. This procedure efficiently generates samples from the posterior distribution, making the construction of prediction regions computationally inexpensive. This aspect is particularly advantageous when dealing with large datasets or complex models, as it allows for scalable implementation of the CB method.\n","\n","In summary, the Conformal Bayesian method outperforms traditional Bayesian approaches in terms of coverage accuracy, robustness to misspecified priors, and computational efficiency. Its ability to provide calibrated uncertainty estimates for individual predictions makes it a valuable tool in decision-making scenarios. The combination of conformal prediction principles with Bayesian inference, along with the utilization of the AOI sampling procedure, offers a powerful framework for reliable uncertainty quantification. Therefore, the CB method should be considered as a preferred approach in various machine learning and statistical applications."],"metadata":{"id":"doe6VGnEGC4a"}}]}